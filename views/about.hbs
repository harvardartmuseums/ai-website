<section class="about">
  <div class="content">
    <h2>About the AI Explorer</h2>
    <div class="red-line-break about-break"></div>
    <div class="about-banner">
      <div class="about-overview">
        <h3>Overview</h3>
        <p>Starting in 2016 the Harvard Art Museums department of Digital Infrastructure and Emerging Technology (DIET) began using artificial intelligence to describe the museums collection. Since then DIET has built a research dataset of {{annotation_count}} machine-generated descriptions and tags covering {{image_count}} images of artworks. Ranging from feature recognition to face analysis that predicts gender, age, and emotion, the data reveals how computers interpret paintings, photographs, and sculptures. This website allows users to explore the extensive collection of data by searching for artworks by machine-generated keyword and looking at aggregated data for individual pieces.</p>
      </div>
      <div class="about-image-container">
        <img class="about-image" src="/images/aboutbanner.png">
      </div>
    </div>
    <div class="about-why-how">
      <div class="about-why-how-text">
        <h3>Why?</h3>
        <p>The Harvard Art Museums is using computer vision and AI for two primary reasons. The first is to categorize, tag, describe, and annotate its collection of art pieces in ways that the staff of curators don't. Since the computer lacks any context or formal training in art history, the machine views and annotates our collection as if walking into an art museum for the first time. The perspective offered by AI leans closer to reflecting the public rather than experts. Currently, the <a href="https://www.harvardartmuseums.org/collections">Harvard Art Museums’ search interface</a> relies on descriptions written by art historians. The addition of AI-generated annotations to our database makes the Harvard Art Museums’ art collection more accessible to non-specialists.</p>
        <p>The second reason is to build a dataset for researching how AI services operate. All of the services we use are <a href="https://en.wikipedia.org/wiki/Black_box">black boxes</a>. This means the services do not disclose the algorithms and training sets used in their systems so we are left to guess how they operate. We use them and provide the data, in part, to call attention to the differences and biases inherent in AI services. </p>
      </div>
      <div class="about-why-how-text">
        <h3>How?</h3>
        <p>The Harvard Art Museums has collected artificially-generated data on artworks from five different AI and computer-vision services: <a href="https://aws.amazon.com/rekognition/">Amazon Rekognition</a>, <a href="https://www.clarifai.com/">Clarifai</a>, <a href="https://imagga.com/">Imagga</a>, <a href="https://cloud.google.com/vision/">Google Vision</a>, and <a href="https://azure.microsoft.com/en-us/services/cognitive-services/">Microsoft Cognitive Services</a>. For each art piece, these services provide interpretations otherwise known as “annotations” that include generated tags and captions and object, face, and text recognition. When a user searches for a keyword, this site takes the user-inputted keyword and finds artworks that contain a matching machine-generated tag. From there, the user can go to an individual piece to see and compare the annotations from the five AI services.</p>
      </div>
      <div class="about-why-how-text">
        <h3>What?</h3>
        <p>We've learned a bit about our collections thanks to these services. Hear about what we've discovered in the presentation, ‘Elephants on Parade or: A Cavalcade of Discoveries from Five CV Systems‘, given by <a href="https://harvardartmuseums.org/about/staff/109">Jeff Steward</a> at the <a href="https://www.aeolian-network.net/events/workshop-2/">AEOLIAN Network workshop</a> ‘Reimagining Industry / Academic / Cultural Heritage Partnerships in AI’ on Monday, October 25, 2021.</p>
        <iframe width="100%" height="400" src="https://www.youtube-nocookie.com/embed/1smG2aGZImY" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
      </div>      
    </div>
    <div class="about-why-how">
      <div class="about-why-how-text">
        <h3>Additional Reading, Viewing, and Listening</h3>
        <p><a href="https://plot.online/plot/points/the-pig-and-the-algorithm/">The Pig and the Algorithm</a><br> Kate Palmer Albers, <em>Plot</em>, March 4, 2017</p>
        <p><a href="https://www.aeolian-network.net/case-study-2-computer-vision-and-cultural-heritage/">Computer Vision and Cultural Heritage: A Case Study</a><br> Catherine Nicole Coleman, AEOLIAN Network, April 2022</p>
        <p><a href="https://www.jbe-platform.com/content/journals/10.1075/idj.22013.rod">Surprise Machines</a><br> Rodighiero, Dario, Lins Derry, Douglas Duhaime, Jordan Kruguer, Maximilian C. Mueller, Christopher Pietsch, Jeffrey T. Schnapp, Jeff Steward, and metaLAB, <em>Information Design Journal</em>, December 2022</p>
        <p><a href="https://youtu.be/OOJN-PSYC_0">Dreaming of AI: Perspectives on AI Use in Cultural Heritage (YouTube)</a><br>BPOC Webinar, September 29, 2023</p>
      </div>            
    </div>
  </div>
</section>
<section class="explore-block-about">
  <div class="content">
    <div class="explore-content">
      <h2>Start exploring</h2>
    </div>
    <div class="explore-panel">
      <div class="explore-panel-content">
        <div class="explore-image-block">
          {{#each image_list}}
            <a href="/object/{{objectid}}"><div class="explore-image" style="background-image:url({{imageurl}}"></div></a>
          {{/each}}
        </div>
        <div class="tag-search">
          <div class="explore-subhead">
            <h3>Annotation Search</h3>
            <p>Use the search bar to find artworks that contain the inputted keyword</p>
          </div>
          <div class="search-container">
            <form method="post" action="/search">
              <input type="text" placeholder="Search here..." name="search">
              <button class="search-button" type="submit"><i class="fa fa-search"></i></button>
            </form>
          </div>
          <div class="example-terms">
            {{#each tag_list}}
              <a href="/search/{{tag}}"><p>{{tag}}</p></a>
            {{/each}}
          </div>
          <div class="example-terms-mobile">
            {{#each mobile_tag_list}}
              <a href="/search/{{tag}}"><p>{{tag}}</p></a>
            {{/each}}
          </div>
        </div>
        <div class="category-search">
          <div class="explore-subhead">
            <h3>Category Search</h3>
            <p>Explore how a machine auto-categorizes images into 12 unique groups</p>
          </div>
          <a href="/category"><button class="link-button gray-button">Category search</button></a>
        </div>
      </div>
    </div>
  </div>
</section>
